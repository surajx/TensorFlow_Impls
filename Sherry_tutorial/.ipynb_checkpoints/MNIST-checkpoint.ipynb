{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from six.moves import xrange\n",
    "import tensorflow as tf\n",
    "\n",
    "# import Dataset\n",
    "from tensorflow.contrib.learn.python.learn.datasets.mnist import read_data_sets\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "NUM_CLASSES = 10  # Classify into each one of 0...9 digits.\n",
    "\n",
    "IMAGE_SIZE = 28  # Square images with resolution 28 x 28\n",
    "IMAGE_PIXELS = IMAGE_SIZE * IMAGE_SIZE\n",
    "\n",
    "# Batch Size\n",
    "BATCH_SIZE = 100  # Must be evenly dividable with the total data size.\n",
    "EVAL_BATCH_SIZE = 1\n",
    "\n",
    "# Hidden Layer\n",
    "HIDDEN_1_UNITS = 128\n",
    "HIDDEN_2_UNITS = 32\n",
    "\n",
    "MAX_STEPS = 2000  # Max Training Steps.\n",
    "\n",
    "TRAIN_DIR = '/tmp/mnist'  # Dir to put training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Get/Load input data\n",
    "data_sets = read_data_sets(TRAIN_DIR, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Build Inference Graph\n",
    "def dynamic_layer_builder():\n",
    "    pass\n",
    "\n",
    "\n",
    "def get_weight_and_biases(shape):\n",
    "    # Samples from truncated normal are bounded at two stddev to either side\n",
    "    # of the mean. \n",
    "    # Initialization Justifications:\n",
    "    # 1. Random initialization helps break symmetry between learned features.\n",
    "    # 2. Bounded values(truncated) help to control the magnitude of the gradients, resulting in better convergence.\n",
    "    # 3. ReLU adjusted Xavier Initialization var[W_i] = \\sqrt{2/(number of inputs to neuron)} [1][2][3][4]\n",
    "    # [1] https://arxiv.org/pdf/1502.01852.pdf\n",
    "    # [2] http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf\n",
    "    # [3] http://andyljones.tumblr.com/post/110998971763/an-explanation-of-xavier-initialization\n",
    "    # [4] http://deepdish.io/2015/02/24/network-initialization\n",
    "    weights = tf.Variable(\n",
    "        tf.truncated_normal(shape, stddev=math.sqrt(2.0/float(shape[0]))),\n",
    "        name=\"weights\")\n",
    "    biases = tf.Variable(tf.zeros([shape[1]]), name=\"biases\")\n",
    "    return weights, biases\n",
    "\n",
    "\n",
    "def mnist_inference(batch_images):\n",
    "    # Isolate variable names to the context of a single layer.\n",
    "    with tf.name_scope('hidden_layer_1'):\n",
    "        weights, biases = get_weight_and_biases([IMAGE_PIXELS, HIDDEN_1_UNITS])\n",
    "        hidden_1_op = tf.nn.relu(tf.matmul(batch_images, weights) + biases)\n",
    "    with tf.name_scope('hiddel_layer_2'):\n",
    "        weights, biases = get_weight_and_biases(\n",
    "            [HIDDEN_1_UNITS, HIDDEN_2_UNITS])\n",
    "        hidden_2_op = tf.nn.relu(tf.matmul(hidden_1_op, weights) + biases)\n",
    "    with tf.name_scope('logit_layer'):\n",
    "        weights, biases = get_weight_and_biases([HIDDEN_2_UNITS, NUM_CLASSES])\n",
    "        logits = tf.matmul(hidden_2_op, weights) + biases\n",
    "    tf.train.write_graph(\n",
    "        tf.get_default_graph().as_graph_def(),\n",
    "        \"/home/surajx/tmp/mnist_graph\",\n",
    "        \"inference.pbtxt\",\n",
    "        as_text=True)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Build training graph\n",
    "def mnist_train_graph(logits, labels, learning_rate):\n",
    "    labels = tf.to_int64(labels)\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        logits=logits, labels=labels, name='xentropy')\n",
    "    loss = tf.reduce_mean(cross_entropy, name='xentropy_mean')\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "    train = optimizer.minimize(loss, global_step=global_step)\n",
    "\n",
    "    tf.train.write_graph(\n",
    "        tf.get_default_graph().as_graph_def(),\n",
    "        \"/home/surajx/tmp/mnist_graph\",\n",
    "        \"train.pbtxt\",\n",
    "        as_text=True)\n",
    "    return train, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Build graph for data input\n",
    "mnist_graph = tf.Graph()\n",
    "with mnist_graph.as_default():\n",
    "    images_placeholder = tf.placeholder(tf.float32)\n",
    "    labels_placeholder = tf.placeholder(tf.int32)\n",
    "    tf.add_to_collection(\"images\", images_placeholder)\n",
    "    tf.add_to_collection(\"labels\", labels_placeholder)\n",
    "\n",
    "    logits = mnist_inference(images_placeholder)\n",
    "\n",
    "    tf.add_to_collection(\"logits\", logits)\n",
    "\n",
    "    train, loss = mnist_train_graph(logits, labels_placeholder, 0.01)\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    tf.train.write_graph(\n",
    "        tf.get_default_graph().as_graph_def(),\n",
    "        \"/home/surajx/tmp/mnist_graph\",\n",
    "        \"complete.pbtxt\",\n",
    "        as_text=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: loss = 2.39\n",
      "Step 1000: loss = 0.27\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Train the graph\n",
    "with tf.Session(graph=mnist_graph) as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for step in xrange(MAX_STEPS):\n",
    "        images_feed, labels_feed = data_sets.train.next_batch(BATCH_SIZE)\n",
    "        _, loss_value = sess.run(\n",
    "            [train, loss],\n",
    "            feed_dict={\n",
    "                images_placeholder: images_feed,\n",
    "                labels_placeholder: labels_feed\n",
    "            })\n",
    "        if step % 1000 == 0:\n",
    "            print('Step %d: loss = %.2f' % (step, loss_value))\n",
    "    checkpoint_file = os.path.join(TRAIN_DIR, 'checkpoint')\n",
    "    saver.save(sess, checkpoint_file, global_step=step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/mnist/checkpoint-1999\n",
      "Ground truth: 9\n",
      "Prediction: 9\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUoQfSIrwA0kRfiCp/we4eTNrlX9/KwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc6e87487f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with tf.Session(graph=tf.Graph()) as sess:\n",
    "    saver = tf.train.import_meta_graph(\n",
    "        os.path.join(TRAIN_DIR, \"checkpoint-1999.meta\"))\n",
    "    saver.restore(sess, os.path.join(TRAIN_DIR, \"checkpoint-1999\"))\n",
    "\n",
    "    # Retrieve the Ops we 'remembered'.\n",
    "    logits = tf.get_collection(\"logits\")[0]\n",
    "    images_placeholder = tf.get_collection(\"images\")[0]\n",
    "    labels_placeholder = tf.get_collection(\"labels\")[0]\n",
    "\n",
    "    # Add an Op that chooses the top k predictions.\n",
    "    eval_op = tf.nn.top_k(logits)\n",
    "\n",
    "    # Run evaluation.\n",
    "#     for step in xrange(20):\n",
    "    images_feed, labels_feed = data_sets.validation.next_batch(EVAL_BATCH_SIZE)\n",
    "    imgplot = plt.imshow(np.reshape(images_feed, (28, 28)))\n",
    "    prediction = sess.run(\n",
    "        eval_op,\n",
    "        feed_dict={\n",
    "            images_placeholder: images_feed,\n",
    "            labels_placeholder: labels_feed\n",
    "        })\n",
    "    print(\"Ground truth: %d\\nPrediction: %d\" % (labels_feed,\n",
    "                                                prediction.indices[0][0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
